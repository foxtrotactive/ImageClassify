diff --git a/train.py b/train.py
index 1234567..abcdefg 100644
--- a/train.py
+++ b/train.py
@@ -1,66 +1,131 @@
+import os
 import torch
-from torch import nn, optim
-from torchvision import datasets, transforms, models
+import cv2
+import numpy as np
+from torch import nn, optim, Tensor
 from torch.utils.data import DataLoader
+from ultralytics import YOLO
+import albumentations as A
+from albumentations.pytorch import ToTensorV2
+from flask import Flask, request, jsonify
 
-# Define data directories
-data_dir = './data'
-train_dir = f'{data_dir}/train'
-test_dir = f'{data_dir}/test'
+# Configuration
+num_classes = 10  # Update with actual number of product categories
+img_size = 640
+batch_size = 32
+device = 'cuda' if torch.cuda.is_available() else 'cpu'
 
-# Define transforms with image normalization
-train_transform = transforms.Compose([
-    transforms.Lambda(lambda x: x.convert('RGB')),  # Ensure RGB format
-    transforms.Resize(256),
-    transforms.RandomResizedCrop(224),
-    transforms.RandomHorizontalFlip(),
-    transforms.ToTensor(),
-    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
+# Custom YOLOv5 model with additional classification layers
+class RetailYOLO(nn.Module):
+    def __init__(self, base_model, num_classes):
+        super().__init__()
+        self.yolo = base_model
+        # Add custom classification head
+        self.fc = nn.Sequential(
+            nn.Linear(512, 256),
+            nn.ReLU(),
+            nn.Dropout(0.2),
+            nn.Linear(256, num_classes)
+        )
+
+    def forward(self, x):
+        features = self.yolo(x)
+        return self.fc(features[-1].mean(dim=[2,3]))
+
+# Albumentations augmentations with low-light handling
+train_transform = A.Compose([
+    A.Resize(img_size, img_size),
+    A.RandomResizedCrop(img_size, img_size, scale=(0.8, 1.2)),
+    A.HorizontalFlip(p=0.5),
+    A.VerticalFlip(p=0.2),
+    A.RandomBrightnessContrast(p=0.3),
+    A.CLAHE(p=0.3),
+    A.HueSaturationValue(p=0.2),
+    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
+    ToTensorV2()
+], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))
+
+test_transform = A.Compose([
+    A.Resize(img_size, img_size),
+    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
+    ToTensorV2()
+], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))
+
+# YOLO Dataset Loader
+class YOLODataset(torch.utils.data.Dataset):
+    def __init__(self, img_dir, label_dir, transform=None):
+        self.img_dir = img_dir
+        self.label_dir = label_dir
+        self.transform = transform
+        self.img_files = [f for f in os.listdir(img_dir) if f.endswith('.jpg')]
+
+    def __len__(self):
+        return len(self.img_files)
+
+    def __getitem__(self, idx):
+        img_name = self.img_files[idx]
+        img_path = os.path.join(self.img_dir, img_name)
+        label_path = os.path.join(self.label_dir, img_name.replace('.jpg', '.txt'))
+
+        image = cv2.imread(img_path)
+        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
+
+        # Load YOLO format labels
+        with open(label_path, 'r') as f:
+            labels = []
+            for line in f:
+                class_label, *coords = line.strip().split()
+                labels.append((float(class_label), *map(float, coords)))
+
+        labels = np.array(labels)
+        class_labels = labels[:, 0].astype(int)
+        bboxes = labels[:, 1:].astype(np.float32)
+
+        if self.transform:
+            transformed = self.transform(
+                image=image,
+                bboxes=bboxes,
+                class_labels=class_labels
+            )
+            image = transformed['image']
+            bboxes = transformed['bboxes']
+            class_labels = transformed['class_labels']
+
+        target = {
+            'boxes': torch.as_tensor(bboxes, dtype=torch.float32),
+            'labels': torch.as_tensor(class_labels, dtype=torch.long)
+        }
+
+        return image, target
+
+# Initialize model with transfer learning
+base_model = YOLO('yolov5s.pt').model
+model = RetailYOLO(base_model, num_classes).to(device)
+
+# Optimize for edge devices
+model = model.half()  # Half-precision for Jetson Nano
+optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)
+scaler = torch.cuda.amp.GradScaler()  # Mixed precision training
+
+# Training loop with validation
+for epoch in range(10):
+    model.train()
+    for batch_idx, (images, targets) in enumerate(train_loader):
+        images = images.half().to(device)
+        
+        with torch.cuda.amp.autocast():
+            outputs = model(images)
+            loss = criterion(outputs, targets)
+        
+        scaler.scale(loss).backward()
+        scaler.step(optimizer)
+        scaler.update()
+        optimizer.zero_grad()
+
+# Flask deployment
+app = Flask(__name__)
+model.eval()
+
+@app.route('/detect', methods=['POST'])
+def detect():
+    img = request.files['image'].read()
+    img = cv2.imdecode(np.frombuffer(img, np.uint8), cv2.IMREAD_COLOR)
+    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
+    
+    with torch.no_grad():
+        results = model(img)
+    
+    return jsonify(results.pandas().xyxy[0].to_dict())
+
+if __name__ == '__main__':
+    app.run(host='0.0.0.0', port=5000)
